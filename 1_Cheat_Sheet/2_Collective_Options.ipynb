{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaconda Command Prompt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "F:\n",
    "jupyter notebook\n",
    "\n",
    "\n",
    "cd F:\\03. Suresh\\1. Material\\05. Data Science\\20. Internship\\6. Technocolabs\\4.Deployment\\Mini_Project\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Flask Web App:\n",
    "\n",
    "F:\n",
    "python app.py\n",
    "\n",
    "-----------------------\n",
    "\n",
    "heroku\n",
    "heroku login\n",
    "\n",
    "heroku logs --app predicttweetbothuman\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "http://127.0.0.1:5000/\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "pip freeze>requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Loading the \"NewCustomerList\" sheet in dataset\n",
    "\n",
    "NewCustomerList = pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name=\"NewCustomerList\", header=1)\n",
    "\n",
    "NewCustomerList = pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", \"NewCustomerList\", header=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Checking size of dataset\n",
    "display(data.shape)\n",
    "display(name.shape)\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "print(len(data))\n",
    "print(len(name))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check for duplicated values\n",
    "df.duplicated().sum()\n",
    "\n",
    "df = df.drop_duplicates(inplace=True)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Rename Column Names:\n",
    "\n",
    "name = pd.read_csv(\"scrip_names_till_2020_09_03.csv\")\n",
    "name.head()\n",
    "\n",
    "\n",
    "id\tname\tzerodha_id\texchange\n",
    "0\t1\tGLENMARK\t1895937\tNSE\n",
    "1\t2\tINDUSINDBK\t1346049\tNSE\n",
    "2\t3\tTECHM\t3465729\tNSE\n",
    "3\t4\tKOTAKBANK\t492033\tNSE\n",
    "4\t5\tRELIANCE\t738561\tNSE\n",
    "\n",
    "\n",
    "\n",
    "name.columns = ['scrip_id', 'name', 'zerodha_id', 'exchange']\n",
    "\n",
    "name.head()\n",
    "\n",
    "\tscrip_id\tname\tzerodha_id\texchange\n",
    "0\t1\tGLENMARK\t1895937\tNSE\n",
    "1\t2\tINDUSINDBK\t1346049\tNSE\n",
    "2\t3\tTECHM\t3465729\tNSE\n",
    "3\t4\tKOTAKBANK\t492033\tNSE\n",
    "4\t5\tRELIANCE\t738561\tNSE\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "name = name.rename(columns={\"id\":\"scrip_id\"})\n",
    "name.head()\n",
    "\n",
    "********************************************************************\n",
    "\n",
    "train.rename(columns={'Country_Region':'Country'}, inplace=True)\n",
    "test.rename(columns={'Country_Region':'Country'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s append the train and test data\n",
    "df = train.append(test, sort=False)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the DataFrames on top of each other\n",
    "# both gives same result\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9])\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9], axis=0)\n",
    "\n",
    "# Place the DataFrames side by side\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glucose, Insulin and SkinThickness should not contain \"0\". So replace \"0\" with median values (mean not used if outliers)\n",
    "import numpy as np\n",
    "df['Glucose'] = np.where(df['Glucose']==0, df['Glucose'].median(), df['Glucose'])\n",
    "df['Insulin'] = np.where(df['Insulin']==0, df['Insulin'].median(), df['Insulin'])\n",
    "df['SkinThickness'] = np.where(df['SkinThickness']==0, df['SkinThickness'].median(), df['SkinThickness'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "df['Gender'] = df['Gender'].fillna(df['Gender'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cr = classification_report(y_pred,y_test)\n",
    "print('Classification Report\\n',cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers ---> mean, mode ---> Normal distributed curve\n",
    "skewness ---> sqr root, cube\n",
    "\n",
    "skewness=0 ---> Normal distributed curve\n",
    "\n",
    "reciprocal ---> -ve skewness (%50 -ve values)\n",
    "\n",
    "Left skew ---> will work on projects very rarely\n",
    "\n",
    "- for categorical features ---> after encoding need to check skewness\n",
    "\n",
    "     - yes / No; Male / Female ---> no need to check skewness\n",
    "  \n",
    "     - more categorical features --- > need to check skewness\n",
    "     \n",
    "     - OneHotEncoding  --->  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. General"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "     sns.countplot(final['account_category'])\n",
    "     \n",
    "     final['account_category'].value_counts().sort_values(ascending=True).plot.bar()\n",
    "     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(train.head())\n",
    "display(test.head())\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "# checking dimension (num of rows and columns) of dataset\n",
    "print(\"Training data shape (Rows, Columns):\",train.shape)\n",
    "print(\"Test data shape (Rows, Columns):\",test.shape)\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "home_data['YearBuilt'].value_counts().sort_index(ascending=False).head()\n",
    "\n",
    "2010     1\n",
    "2009    18\n",
    "2008    23\n",
    "2007    49\n",
    "2006    67\n",
    "Name: YearBuilt, dtype: int64\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "home_data['YearBuilt'].value_counts().sort_values(ascending=False).head()\n",
    "\n",
    "2006    67\n",
    "2005    64\n",
    "2004    54\n",
    "2007    49\n",
    "2003    45\n",
    "Name: YearBuilt, dtype: int64\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Numeric Features:\n",
    "\n",
    "      numeric_cols_train = train.select_dtypes(include=[np.number])\n",
    "      display(numeric_cols_train.head())\n",
    "      print('\\n')\n",
    "      numeric_cols_train.columns\n",
    "      \n",
    "Categorical Features\n",
    "\n",
    "      categorical_cols_train = train.select_dtypes(include=[np.object])\n",
    "      display(categorical_cols_train.head())\n",
    "      print('\\n')\n",
    "      categorical_cols_train.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_new['Dependents'] = train_new['Dependents'].astype('int64') \n",
    "train_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. OPENCV:\n",
    "\n",
    "     cv2.__version__\n",
    "     \n",
    "     conda install -c conda-forge opencv\n",
    "     \n",
    "     pip install opencv-python\n",
    "     \n",
    "     \n",
    "2. Pytesseract:\n",
    "\n",
    "     import pytesseract\n",
    "\n",
    "     pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\deepusuresh\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "     \n",
    "     conda install -c conda-forge pytesseract\n",
    "     \n",
    "     \n",
    "3. gluonnlp\n",
    "\n",
    "     import gluonnlp as nlp\n",
    "     \n",
    "     conda install -c anaconda mxnet\n",
    "     \n",
    "     \n",
    "4. tweepy\n",
    "\n",
    "     import tweepy\n",
    "     \n",
    "     conda install -c conda-forge tweepy\n",
    "     \n",
    "     \n",
    "5. NLTK\n",
    "\n",
    "     import nltk\n",
    " \n",
    "     conda install -c anaconda nltk\n",
    "     \n",
    "     \n",
    "6. Joblib\n",
    "\n",
    "     conda install -c anaconda joblib\n",
    "     \n",
    "     import joblib\n",
    "     \n",
    "\n",
    "7. rdkit\n",
    "\n",
    "     conda install -c rdkit rdkit\n",
    "     \n",
    "     conda create -c rdkit -n my-rdkit-env rdkit\n",
    "   \n",
    "  -------------------------------------------------------- \n",
    "     \n",
    "     To activate this environment, use:\n",
    "     \n",
    "             conda activate my-rdkit-env\n",
    "             \n",
    "     To deactivate an active environment, use:\n",
    "     \n",
    "             conda deactivate\n",
    "             \n",
    "  -------------------------------------------------------\n",
    "     \n",
    "     from rdkit import rdBase\n",
    "     \n",
    "     from rdkit import Chem\n",
    "     \n",
    "  -------------------------------------------------------\n",
    "  \n",
    "     To register the my-rdkit-env with Jupyter run the following command:\n",
    "     \n",
    "            conda install -n my-rdkit-env nb_conda_kernels\n",
    "     \n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Deployment:\n",
    "\n",
    "\n",
    "1. flask\n",
    "\n",
    "     conda install -c anaconda flask\n",
    "     \n",
    "     conda install flask\n",
    "     \n",
    "     pip install flask\n",
    "     \n",
    "     print(flask.__version__)\n",
    "     \n",
    "\n",
    "2. Gunicorn\n",
    "\n",
    "     conda install -c anaconda gunicorn\n",
    "     \n",
    "     conda install -c conda-forge gunicorn\n",
    "     \n",
    "     pip install gunicorn\n",
    "     \n",
    "     \n",
    "3. werkzeug\n",
    "\n",
    "     conda install -c anaconda werkzeug\n",
    "     \n",
    "     \n",
    "4. streamlit\n",
    "\n",
    "     conda install -c conda-forge streamlit\n",
    "     \n",
    "     pip install streamlit\n",
    "     \n",
    "     streamlit run iris_webapp.py\n",
    "     \n",
    "     pip install --upgrade protobuf\n",
    "     \n",
    "     \n",
    "5. flasgger\n",
    "\n",
    "     conda install -c conda-forge flasgger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summary and count for categorical attribute\n",
    "\n",
    "       data.describe(include=[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "x_df = pd.DataFrame(df)\n",
    "print(x_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c_col['country'].unique()\n",
    "\n",
    "c_col['country'].nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.cont()                         ---->     Gives Column wise count\n",
    "\n",
    "df['Gender'].unique()             ---->     Gives in a column how many types of strings are there\n",
    "\n",
    "df['Gender'].nunique()            ---->     Gives in a column total number of strings are there\n",
    "\n",
    "df['Gender'].value_counts()       ---->     Gives total no of values in a particular column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. EDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = train_df[['area_type', 'availability' ,'size', 'total_sqft', 'bath', 'balcony']]\n",
    "y = train_df[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Dependents'] = train['Dependents'].replace(\"3+\",3)\n",
    "test['Dependents'] = test['Dependents'].replace(\"3+\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Married = train.Married.map({'Yes':1, 'No':0})\n",
    "test.Married = test.Married.map({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Education = train.Education.map({'Graduate':1, 'Not Graduate':0})\n",
    "test.Education = test.Education.map({'Graduate':1, 'Not Graduate':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major',\n",
    "                                       'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Smith')\n",
    "\n",
    "df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Mme', 'Mrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values in each columns\n",
    "for i in catg_col:\n",
    "  print(i + '    ',catg_col[i].unique())\n",
    "\n",
    "rbc     ['normal' 'abnormal']\n",
    "pc     ['normal' 'abnormal']\n",
    "pcc     ['notpresent' 'present']\n",
    "ba     ['notpresent' 'present']\n",
    "htn     ['yes' 'no']\n",
    "dm     ['yes' 'no' ' yes' '\\tno' '\\tyes']\n",
    "cad     ['no' 'yes' '\\tno']\n",
    "appet     ['good' 'poor']\n",
    "pe     ['no' 'yes']\n",
    "ane     ['no' 'yes']\n",
    "classification_en     [1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before applying feature encoding on these columns. If we look properly we need to do some cleaning here. There are values present as **'\\tno', '\\tyes' in dm and cad.** There appears to be a **typo error**. Wo e have to **replace '\\tno' with no and 'tyes' with yes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and strip the values\n",
    "catg_col['dm'] = catg_col['dm'].str.strip()\n",
    "catg_col['cad'] = catg_col['dm'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df.loc[df['language']=='English']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encoding Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables are of 2 types Ordinal and Nominal. \n",
    "\n",
    "*   Ordinal variables has some kind order. (Good, Better, Best), (First, Second, Third)\n",
    "\n",
    "\n",
    "*   Nominal variables has no ordering between them. (Cat, Dog, Monkey), (Apple, Banana, Mango)\n",
    "\n",
    "Based on categorical variables whether they are ordinal or nominal we appply different techniques on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Here the labels are nominal; we will apply OneHotEncoding here**.\n",
    "\n",
    "\n",
    "- we will apply **Binary encoding** on 3 columns as they are **nominal categorical variable ('market_segment', 'assigned_room_type','reserved_room_type')**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Label Encoder:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # import the LabelEncoder from sklrean library\n",
    "le = LabelEncoder()    # create the instance of LabelEncoder\n",
    "\n",
    "df['country_temp'] = le.fit_transform(df['country'])   # apply LabelEncoding of country column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies (OHE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# drop_first=True ----> used to avoid dummy variable trap\n",
    "\n",
    "salary_dummies = pd.get_dummies(df2['salary'], drop_first=True)\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "# Transform discrete values to columns with 1 and 0s\n",
    "train_OHE = pd.get_dummies(train)\n",
    "\n",
    "# Do the same for competition data\n",
    "test_OHE = pd.get_dummies(test)\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "display(train_OHE.head())\n",
    "display(test_OHE.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install category_encoders\n",
    "\n",
    "# install category_encoders first\n",
    "# You have to poen your ananconda prompt & conda install -c conda-forge category_encoders & Y/N,Y\n",
    "\n",
    "\n",
    "import category_encoders as ce\n",
    "encoder = ce.BinaryEncoder()\n",
    "b_encoder = encoder.fit_transform(obj_df['make'])\n",
    "b_encoder.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Some styling\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "sns.set()   # set visualisation style\n",
    "import matplotlib.style as style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for loop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in Categorical:\n",
    "    print(i + '     ', Categorical[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PANDAS Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['Open'].plot(figsize=(16,6))\n",
    "\n",
    "boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Matplotlib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.hist(df.strength)\n",
    "plt.show()\n",
    "\n",
    "      (or)\n",
    "      \n",
    "df.hist(figsize = (10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (8,4)\n",
    "\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10,4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Set Plotting parameters\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10,4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Seaborn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 7, 6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "data['price'].hist(grid = False)\n",
    "\n",
    "         (or)\n",
    "         \n",
    "# density plot\n",
    "sns.distplot(data['price'], hist = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Count Plot:\n",
    "\n",
    "\n",
    " ------------------------------------------------------------------------------------------------------------------------\n",
    " \n",
    "    sns.countplot('Class', data=df)                          #  Default count will put in Vertical axis\n",
    "    \n",
    " ------------------------------------------------------------------------------------------------------------------------\n",
    " \n",
    "    sns.countplot(x='Class', data=df)                        #  x ='Class' means count will put in Vertical axis\n",
    "    \n",
    "                   (or)\n",
    "                   \n",
    "    sns.countplot(x=df['Class'], data=df)\n",
    "    \n",
    "                   (or)\n",
    "    \n",
    "    sns.countplot(df['Class'], data=df)\n",
    "    \n",
    " ------------------------------------------------------------------------------------------------------------------------   \n",
    "      \n",
    "    sns.countplot(y ='Class', data=df)                       #  y ='Class' means count will put in horizontal axis\n",
    "      \n",
    " ------------------------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "    sns.countplot(Service['Complaint Type'])                 #  Service  ----> Dataframe\n",
    "    \n",
    " ------------------------------------------------------------------------------------------------------------------------   \n",
    " \n",
    "    plot = sns.countplot(df['Complaint Type'], xticklabels='auto')\n",
    "    plot.set_xticklabels(plot.get_xticklabels(),rotation=90)\n",
    "    \n",
    " ------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Heatmap:\n",
    "\n",
    "   1. Missing values & Heatmap\n",
    "\n",
    "          plt.figure(figsize=(17,10))\n",
    "          sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "          \n",
    "          \n",
    "          fmt='.2g'\n",
    "          \n",
    "          fmt='d'\n",
    "          \n",
    "          fmt='.1f'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pairplot:\n",
    "\n",
    "          \n",
    "          sns.pairplot(train)\n",
    "          \n",
    "          sns.pairplot(train, hue='Sex')\n",
    "          \n",
    "          sns.pairplot(train, height = 2, kind ='scatter', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Bar plot (catplot) (univariate & bivariate analysis)\n",
    "\n",
    "\n",
    "          sns.catplot(x='Pclass', y='Fare', data=train, kind='bar')\n",
    "          \n",
    "          sns.catplot(x='Pclass', y='Survived', hue='Sex', data=train, kind='bar')\n",
    "          \n",
    "          sns.catplot(x='Survived', col='Embarked', kind='count', data=train)\n",
    "          \n",
    "          sns.catplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train, kind='bar')\n",
    "          \n",
    "          \n",
    "          \n",
    "# Box Plot (univariate & bivariate analysis)\n",
    "\n",
    "    1. Univariate\n",
    "    \n",
    "          sns.boxplot(data=train['SalePrice'], palette='winter')\n",
    "          \n",
    "          df.boxplot(column='Positive Skewed')                     # using Pandas\n",
    "          \n",
    "    2. Bivariate\n",
    "          \n",
    "          sns.boxplot(x='Pclass', y='Age', data=train, palette='winter')\n",
    "          \n",
    "          sns.boxplot(x='Sex', y='Age', data=train)\n",
    "          \n",
    "          sns.boxplot(x='Sex', y='Age', data=train, hue='Survived')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Box Plot:\n",
    "\n",
    "\n",
    "          df.boxplot(return_type='dict')\n",
    "\n",
    "          plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scatter Plot\n",
    "\n",
    "          - scatter requires x & y columns to be numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plotly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ML"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1\n",
    "## H2\n",
    "### H3\n",
    "\n",
    "**bold**\n",
    "\n",
    "_titanic_\n",
    "\n",
    "* Bullet piont\n",
    "* Bullet point\n",
    "\n",
    "Read more: https://www.youtube.com/watch?v=A3gClkblXK8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short cuts:\n",
    "\n",
    "\n",
    "   - convert Code ----> Markdown         ------     M\n",
    "   \n",
    "   \n",
    "   - convert Markdown ----> Code         ------     Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train.shape , test.shape\n",
    "\n",
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data preparation\n",
    "\n",
    "   Dummy variables\n",
    "   Train/test split\n",
    "   Cross Validation Setting\n",
    "   Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA)\n",
    "\n",
    "      Exploratory Data Analysis is the examination of data and find out relationships among variables through both numerical \n",
    "      \n",
    "      and graphical methods. EDA is a task of analyze data, investigate data to the way we find out patterns, relationship, \n",
    "      \n",
    "      outliers and distribution of data. It is one of the most important task for data scienctist to do data science task.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "      \n",
    "1. Basic Analysis with Pandas\n",
    "\n",
    "      A. Descriptive analysis\n",
    "      \n",
    "2. Missing Values\n",
    "\n",
    "3. Numerical Features\n",
    "\n",
    "      A. Univariate Analysis\n",
    "      \n",
    "      B. Bivariate Analysis or Correlation coefficients between numeric features and dependent feature\n",
    "\n",
    "      C. Outliers of numerical features\n",
    "\n",
    "4. Categorical Features\n",
    "\n",
    "      A. Bivariate Analysis or Correlation coefficients between Categorical features and dependent feature\n",
    "      \n",
    "5. Central tendency and distribution of target columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Disable warnings\n",
    " \n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import the necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot \n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
    "\n",
    "!pip install pywaffle\n",
    "from pywaffle import Waffle\n",
    "py.init_notebook_mode(connected=True)\n",
    "import folium \n",
    "from folium import plugins\n",
    "plt.style.use(\"fivethirtyeight\")# for pretty graphs\n",
    "\n",
    "# Increase the default plot size and set the color scheme\n",
    "plt.rcParams['figure.figsize'] = 8, 5\n",
    "#plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Disable warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get the column names of NYC data\n",
    "\n",
    "df.columns\n",
    "   (OR)\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checking for total null values\n",
    "# axis = 0 for column wise analysis\n",
    "\n",
    "     print(df.isnull().sum(axis = 0))\n",
    "\n",
    "\n",
    "# how many variables in an observation have null values\n",
    "\n",
    "     print(df.isnull().sum(axis = 1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"311_Service_Requests_from_2010_to_Present.csv\")\n",
    "df.head()\n",
    "\n",
    "                (or)\n",
    "                \n",
    "df = pd.read_csv(\"311_Service_Requests_from_2010_to_Present.csv\")\n",
    "pd.options.display.max_columns = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-28 11:45:07.452449\n"
     ]
    }
   ],
   "source": [
    "x = datetime.datetime.now()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April\n",
      "Apr\n",
      "Tue\n",
      "Tuesday\n",
      "28\n",
      "04\n",
      "20\n",
      "2020\n",
      "11\n",
      "AM\n",
      "45\n",
      "07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.strftime(\"%B\"))\n",
    "print(x.strftime(\"%b\"))\n",
    "print(x.strftime(\"%a\"))\n",
    "print(x.strftime(\"%A\"))\n",
    "print(x.strftime(\"%d\"))\n",
    "print(x.strftime(\"%m\"))\n",
    "print(x.strftime(\"%y\"))\n",
    "print(x.strftime(\"%Y\"))\n",
    "print(x.strftime(\"%H\"))\n",
    "print(x.strftime(\"%p\"))\n",
    "print(x.strftime(\"%M\"))\n",
    "print(x.strftime(\"%S\"))\n",
    "print(x.strftime(\"%Z\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['Complaint Type'].value_counts().head()\n",
    "\n",
    "             (or)\n",
    "    \n",
    "df['Complaint Type'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.dropna())\n",
    "\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'' scikit ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bullet Points:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- point1\n",
    "- point2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- point1\n",
    "- point2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__Step 1:__ Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Import Packages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- A pairs plot allows us to see both __distribution of single variables__ and **relationships between two variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A pairs plot allows us to see both __distribution of single variables__ and **relationships between two variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 2E37-8D69\n",
      "\n",
      " Directory of C:\\Users\\deepusuresh\\Documents\\Data Science\\01. Python\n",
      "\n",
      "30-03-2020  10:27    <DIR>          .\n",
      "30-03-2020  10:27    <DIR>          ..\n",
      "30-03-2020  10:27    <DIR>          .ipynb_checkpoints\n",
      "26-03-2020  00:43    <DIR>          1. Practice\n",
      "11-03-2020  21:11    <DIR>          2. Project\n",
      "16-09-2019  23:31    <DIR>          3. Simulation Test\n",
      "30-03-2020  10:27                72 Collective Options.ipynb\n",
      "12-03-2020  23:21             1,011 Links.ipynb\n",
      "15-06-2019  10:23    <DIR>          Python_Simplilearn_DayCode\n",
      "08-12-2019  23:49            20,234 Simple Tutorial - How to handle missing data!!.ipynb\n",
      "               3 File(s)         21,317 bytes\n",
      "               7 Dir(s)  146,485,571,584 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p><b># We have a dataset containing prices of used BMW cars.</b></p>\n",
    "<p><b># We are going to analyze this dataset and build a prediction function that can predict a price\n",
    "        by taking mileage and age of the car as input.</b></p>\n",
    "<p><b># We will use sklearn train_test_split method to split training and testing dataset</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b># We have a dataset containing prices of used BMW cars.</b></p>\n",
    "<p><b># We are going to analyze this dataset and build a prediction function that can predict a price\n",
    "        by taking mileage and age of the car as input.</b></p>\n",
    "<p><b># We will use sklearn train_test_split method to split training and testing dataset</b></p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p style='color:purple'><b>The approach we are going to use here is to split available data in two sets</b></p>\n",
    "    <ol>\n",
    "        <b>\n",
    "        <li>Training: We will train our model on this dataset</li>\n",
    "        <li>Testing: We will use this subset to make actual predictions using trained model</li>\n",
    "        </b>\n",
    "     </ol>\n",
    "<p style='color:purple'><b>The reason we don't use same training set for testing is because our model has seen those samples before, using same samples for making predictions might give us wrong impression about accuracy of our model. It is like you ask same questions in exam paper as you tought the students in the class.\n",
    "</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:purple'><b>The approach we are going to use here is to split available data in two sets</b></p>\n",
    "    <ol>\n",
    "        <b>\n",
    "        <li>Training: We will train our model on this dataset</li>\n",
    "        <li>Testing: We will use this subset to make actual predictions using trained model</li>\n",
    "        </b>\n",
    "     </ol>\n",
    "<p style='color:purple'><b>The reason we don't use same training set for testing is because our model has seen those samples before, using same samples for making predictions might give us wrong impression about accuracy of our model. It is like you ask same questions in exam paper as you tought the students in the class.\n",
    "</b></p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Car Mileage Vs Sell Price ($)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Car Mileage Vs Sell Price ($)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](equation.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
